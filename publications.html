<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/f30152c0704fba31.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/4e6a86ce4da87928.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-0f968b0d62f1a266.js"/><script src="/_next/static/chunks/4bd1b696-c4fd0ea3729ebf40.js" async=""></script><script src="/_next/static/chunks/1684-01e159be6f5dc7d4.js" async=""></script><script src="/_next/static/chunks/main-app-d44e26587a51ff49.js" async=""></script><script src="/_next/static/chunks/bc9e92e6-1c042a4cb7f7ee91.js" async=""></script><script src="/_next/static/chunks/457b8330-4b761ccea66ed9aa.js" async=""></script><script src="/_next/static/chunks/aa35ee89-a2c5fbd77e3fd8af.js" async=""></script><script src="/_next/static/chunks/6874-e99f1ffdde1ee6c4.js" async=""></script><script src="/_next/static/chunks/883-1c432fd5a9417496.js" async=""></script><script src="/_next/static/chunks/283-5caa64571be82a90.js" async=""></script><script src="/_next/static/chunks/app/layout-c6fdd31ec17bd76a.js" async=""></script><script src="/_next/static/chunks/app/publications/page-e9c5f60185dda5d2.js" async=""></script><script>
            // Hide any Firebase configuration check messages
            (function() {
              // Text to hide
              const textToHide = 'Kiểm tra cấu hình Firebase';

              // Function to hide Firebase messages
              function hideFirebaseMessages() {
                // Add a class to the body
                if (document.body) {
                  document.body.classList.add('firebase-messages-hidden');
                }

                // Simple approach: hide any element containing the text
                const allElements = document.querySelectorAll('*');
                allElements.forEach(function(el) {
                  if (el.textContent && el.textContent.includes(textToHide)) {
                    el.style.display = 'none';
                  }
                });

                // Also hide elements with data attribute
                const dataElements = document.querySelectorAll('[data-firebase-config-check]');
                dataElements.forEach(function(el) {
                  el.style.display = 'none';
                });
              }

              // Run immediately if possible
              if (document.readyState !== 'loading') {
                hideFirebaseMessages();
              } else {
                document.addEventListener('DOMContentLoaded', hideFirebaseMessages);
              }

              // Also run on load
              window.addEventListener('load', hideFirebaseMessages);

              // Set up interval to keep checking
              setInterval(hideFirebaseMessages, 1000);

              // Set up MutationObserver for dynamic content
              if (typeof MutationObserver !== 'undefined') {
                const observer = new MutationObserver(hideFirebaseMessages);

                // Start observing when body is available
                function setupObserver() {
                  if (document.body) {
                    observer.observe(document.body, {
                      childList: true,
                      subtree: true,
                      characterData: true
                    });
                  } else {
                    setTimeout(setupObserver, 100);
                  }
                }

                setupObserver();
              }
            })();
          </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_5cfdac __variable_9a8899 antialiased" style="background-color:#0a0a0a;color:#ededed;visibility:visible;display:block"><script src="/_next/static/chunks/webpack-0f968b0d62f1a266.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n3:I[283,[\"2992\",\"static/chunks/bc9e92e6-1c042a4cb7f7ee91.js\",\"9507\",\"static/chunks/457b8330-4b761ccea66ed9aa.js\",\"7158\",\"static/chunks/aa35ee89-a2c5fbd77e3fd8af.js\",\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"883\",\"static/chunks/883-1c432fd5a9417496.js\",\"283\",\"static/chunks/283-5caa64571be82a90.js\",\"7177\",\"static/chunks/app/layout-c6fdd31ec17bd76a.js\"],\"AuthProvider\"]\n4:I[4126,[\"2992\",\"static/chunks/bc9e92e6-1c042a4cb7f7ee91.js\",\"9507\",\"static/chunks/457b8330-4b761ccea66ed9aa.js\",\"7158\",\"static/chunks/aa35ee89-a2c5fbd77e3fd8af.js\",\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"883\",\"static/chunks/883-1c432fd5a9417496.js\",\"283\",\"static/chunks/283-5caa64571be82a90.js\",\"7177\",\"static/chunks/app/layout-c6fdd31ec17bd76a.js\"],\"AdminProvider\"]\n5:I[7450,[\"2992\",\"static/chunks/bc9e92e6-1c042a4cb7f7ee91.js\",\"9507\",\"static/chunks/457b8330-4b761ccea66ed9aa.js\",\"7158\",\"static/chunks/aa35ee89-a2c5fbd77e3fd8af.js\",\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"883\",\"static/chunks/883-1c432fd5a9417496.js\",\"283\",\"static/chunks/283-5caa64571be82a90.js\",\"7177\",\"static/chunks/app/layout-c6fdd31ec17bd76a.js\"],\"default\"]\n6:I[1970,[\"2992\",\"static/chunks/bc9e92e6-1c042a4cb7f7ee91.js\",\"9507\",\"static/chunks/457b8330-4b761ccea66ed9aa.js\",\"7158\",\"static/chunks/aa35ee89-a2c5fbd77e3fd8af.js\",\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"883\",\"static/chunks/883-1c432fd5a9417496.js\",\"283\",\"static/chunks/283-5caa64571be82a90.js\",\"7177\",\"static/chunks/app/layout-c6fdd31ec17bd76a.js\"],\"default\"]\n7:I[7555,[],\"\"]\n8:I[1295,[],\"\"]\n9:I[6821,[\"2992\",\"static/chunks/bc9e92e6-1c042a4cb7f7ee91.js\",\"9507\",\"static/chunks/457b8330-4b761ccea66ed9aa.js\",\"7158\",\"static/chunks/aa35ee89-a2c5fbd77e3fd8af.js\",\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"883\",\"static/chunks/883-1c432fd5a9417496.js\",\"283\",\"static/chunks/283-5caa64571be82a90.js\",\"7177\",\"static/chunks/app/layout-c6fdd31ec17bd76a.js\"],\"default\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[9665,[],\"OutletBoundary\"]\n10:I[4911,[],\"AsyncMetadataOutlet\"]\n12:I[9665,[],\"ViewportBound"])</script><script>self.__next_f.push([1,"ary\"]\n14:I[6614,[],\"\"]\n:HL[\"/_next/static/css/f30152c0704fba31.css\",\"style\"]\n:HL[\"/_next/static/css/4e6a86ce4da87928.css\",\"style\"]\n2:T90b,"])</script><script>self.__next_f.push([1,"\n            // Hide any Firebase configuration check messages\n            (function() {\n              // Text to hide\n              const textToHide = 'Kiểm tra cấu hình Firebase';\n\n              // Function to hide Firebase messages\n              function hideFirebaseMessages() {\n                // Add a class to the body\n                if (document.body) {\n                  document.body.classList.add('firebase-messages-hidden');\n                }\n\n                // Simple approach: hide any element containing the text\n                const allElements = document.querySelectorAll('*');\n                allElements.forEach(function(el) {\n                  if (el.textContent \u0026\u0026 el.textContent.includes(textToHide)) {\n                    el.style.display = 'none';\n                  }\n                });\n\n                // Also hide elements with data attribute\n                const dataElements = document.querySelectorAll('[data-firebase-config-check]');\n                dataElements.forEach(function(el) {\n                  el.style.display = 'none';\n                });\n              }\n\n              // Run immediately if possible\n              if (document.readyState !== 'loading') {\n                hideFirebaseMessages();\n              } else {\n                document.addEventListener('DOMContentLoaded', hideFirebaseMessages);\n              }\n\n              // Also run on load\n              window.addEventListener('load', hideFirebaseMessages);\n\n              // Set up interval to keep checking\n              setInterval(hideFirebaseMessages, 1000);\n\n              // Set up MutationObserver for dynamic content\n              if (typeof MutationObserver !== 'undefined') {\n                const observer = new MutationObserver(hideFirebaseMessages);\n\n                // Start observing when body is available\n                function setupObserver() {\n                  if (document.body) {\n                    observer.observe(document.body, {\n                      childList: true,\n                      subtree: true,\n                      characterData: true\n                    });\n                  } else {\n                    setTimeout(setupObserver, 100);\n                  }\n                }\n\n                setupObserver();\n              }\n            })();\n          "])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"2VP04QkNGi4YZX3VS0ew6\",\"p\":\"\",\"c\":[\"\",\"publications\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f30152c0704fba31.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4e6a86ce4da87928.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$2\"}}]}],[\"$\",\"body\",null,{\"className\":\"__variable_5cfdac __variable_9a8899 antialiased\",\"style\":{\"backgroundColor\":\"#0a0a0a\",\"color\":\"#ededed\",\"visibility\":\"visible\",\"display\":\"block\"},\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"children\":[\"$\",\"$L5\",null,{\"children\":[\"$\",\"div\",null,{\"id\":\"app-content\",\"style\":{\"minHeight\":\"100vh\",\"display\":\"flex\",\"flexDirection\":\"column\"},\"children\":[[\"$\",\"$L6\",null,{}],[\"$\",\"main\",null,{\"style\":{\"flex\":\"1 1 auto\"},\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L9\",null,{}]]}]}]}]}]}]]}]]}],{\"children\":[\"publications\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$La\",[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],null,[\"$\",\"$Ld\",null,{\"children\":[\"$Le\",\"$Lf\",[\"$\",\"$L10\",null,{\"promise\":\"$@11\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"HiwE-blhFvE7DKV0qGm4-\",{\"children\":[[\"$\",\"$L12\",null,{\"children\":\"$L13\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"15:\"$Sreact.suspense\"\n16:I[4911,[],\"AsyncMetadata\"]\nc:[\"$\",\"$15\",null,{\"fallback\":null,\"children\":[\"$\",\"$L16\",null,{\"promise\":\"$@17\"}]}]\n"])</script><script>self.__next_f.push([1,"f:null\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script><script>self.__next_f.push([1,"17:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Duc Le | Personal Portfolio\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Welcome to my personal portfolio showcasing my projects, publications, and professional journey.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Duc Le\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"portfolio,developer,projects,publications,personal website\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"Duc Le | Personal Portfolio\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Welcome to my personal portfolio showcasing my projects, publications, and professional journey.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://your-domain.com\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"Duc Le Portfolio\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Duc Le | Personal Portfolio\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"Welcome to my personal portfolio showcasing my projects, publications, and professional journey.\"}],[\"$\",\"link\",\"13\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]],\"error\":null,\"digest\":\"$undefined\"}\n11:{\"metadata\":\"$17:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"18:I[6136,[\"6874\",\"static/chunks/6874-e99f1ffdde1ee6c4.js\",\"8352\",\"static/chunks/app/publications/page-e9c5f60185dda5d2.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"main\",null,{\"className\":\"min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gradient-to-r from-green-600 to-teal-700 text-white py-20 px-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto max-w-5xl\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-bold mb-6 text-center\",\"children\":[\"Academic \",[\"$\",\"span\",null,{\"className\":\"text-yellow-300\",\"children\":\"Publications\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-xl text-center max-w-3xl mx-auto text-green-100\",\"children\":\"Explore my research contributions to the academic community. Each publication represents a significant advancement in knowledge and understanding.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 py-12 -mt-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"bg-white dark:bg-gray-800 rounded-xl shadow-xl p-6 md:p-8 border border-gray-200 dark:border-gray-700 space-y-8\",\"children\":[[\"$\",\"$L18\",null,{\"publications\":[{\"id\":\"from-visual-explanations-to-counterfactual-explanations-with-latent-diffusion\",\"rank\":\"A\",\"title\":\"From Visual Explanations to Counterfactual Explanations with Latent Diffusion\",\"authors\":[\"Tung Luu\",\"Nam Le\",\"Duc Le\",\"Bac Le\"],\"type\":\"Conference\",\"isFirstAuthor\":false,\"highlight\":\"Proposes ECED, a novel framework that unifies visual explanations and latent diffusion to generate semantically precise, realistic, and classifier-aligned counterfactual images without requiring robust classifiers.\",\"venue\":\"IEEE/CVF Winter Conference on Applications of Computer Vision\",\"abbreviation\":\"WACV\",\"year\":\"2025-04-08\",\"abstract\":\"This paper presents ECED, a new counterfactual explanation framework that blends ScoreCAM-based visual attention with blended latent diffusion and adversarial gradient pruning. ECED addresses major challenges in visual counterfactual generation, such as background preservation and dependency on robust classifiers, by operating in the latent space and strategically editing only foreground regions. Extensive evaluations on ImageNet and CelebA-HQ datasets show ECED outperforms prior methods (ACE, DVCE) in sparsity, realism, validity, and interpretability.\",\"problem\":[{\"type\":\"text\",\"text\":\"Visual explanation techniques often highlight overlapping regions between classes, which obscures unique features necessary to distinguish one concept from another.\"},{\"type\":\"text\",\"text\":\"State-of-the-art counterfactual generation methods typically rely on adversarially robust models or cause unwanted image distortions, making explanations less interpretable and less realistic.\"}],\"gap\":[{\"type\":\"text\",\"text\":\"Current approaches lack precise region targeting, often altering irrelevant parts of the image or background during counterfactual generation.\"},{\"type\":\"text\",\"text\":\"Heavy reliance on robust classifiers limits the scalability and generalization of existing counterfactual methods in practical applications.\"}],\"solution\":[{\"type\":\"text\",\"text\":\"ECED combines ScoreCAM to detect class-relevant regions, applies latent-space diffusion only to foreground pixels using a blended latent masking strategy, and prunes adversarial gradients to ensure semantic precision.\"},{\"type\":\"image\",\"url\":\"https://raw.githubusercontent.com/tungluuai/ECED/main/figures/architecture.png\",\"caption\":\"ECED architecture: visual region selection, latent background preservation, and iterative counterfactual editing.\"},{\"type\":\"text\",\"text\":\"The model decouples foreground and background edits, fine-tunes latent embeddings and the decoder to stabilize content reconstruction, and uses gradient-based attacks in the latent space to avoid pixel-level noise.\"}],\"results\":[{\"type\":\"text\",\"text\":\"On ImageNet, ECED outperforms ACE and DVCE in Flip Ratio (FR), Counterfactual Transition (COUT), and FID scores across all tasks like 'Zebra-Sorrel' and 'Cougar-Cheetah'.\"},{\"type\":\"text\",\"text\":\"On CelebA-HQ, ECED achieves the best balance between Face Verification Accuracy, sparsity (MNAC), and realism (FS) for 'Smile' and 'Age' attribute changes.\"}],\"insights\":[{\"type\":\"text\",\"text\":\"Combining ScoreCAM maps with latent-space diffusion edits ensures that generated counterfactuals align closely with both model predictions and human perceptual understanding.\"},{\"type\":\"text\",\"text\":\"Foreground masking and context-aware denoising preserve scene coherence while allowing precise semantic transformation, such as adding wrinkles or changing fur patterns.\"}],\"contributions\":[{\"type\":\"text\",\"text\":\"1. Introduced ECED, which integrates visual and counterfactual explanations using ScoreCAM and latent diffusion without requiring robust classifiers.\"},{\"type\":\"text\",\"text\":\"2. Proposed a background-preserving blending strategy in latent space for efficient, high-fidelity counterfactual generation.\"},{\"type\":\"text\",\"text\":\"3. Delivered superior quantitative and qualitative results on multiple benchmarks while maintaining semantic interpretability and visual quality.\"}],\"topics\":[\"Explainable AI\",\"Counterfactual Explanations\",\"Deep Learning\",\"Computer Vision\",\"Adversarial Machine Learning\",\"Latent Diffusion Models\",\"Visual Explanations\"],\"doi\":\"10.1109/MIPR57485.2024.10123456\",\"links\":{\"website\":null,\"youtube_demo\":null,\"github_repository\":\"https://github.com/tungluuai/ECED\",\"view_publication\":\"https://arxiv.org/pdf/2504.09202\"},\"citationCount\":0,\"citationFormat\":\"Luu, T., Le, N., Le, D., \u0026 Le, B. (2025). From Visual Explanations to Counterfactual Explanations with Latent Diffusion. In Proceedings of IEEE MIPR 2024.\",\"images\":[],\"technologies\":[\"ScoreCAM\",\"Stable Diffusion\",\"Blended Latent Masking\",\"VAE\",\"CLIP\",\"PyTorch\",\"Adam Optimizer\"],\"references\":{\"1\":\"Augustin et al., 'Diffusion Visual Counterfactual Explanations', NeurIPS 2022\",\"2\":\"Avrahami et al., 'Blended Latent Diffusion', TOG 2023\",\"3\":\"Jeanneret et al., 'Adversarial Counterfactual Visual Explanations', CVPR 2023\"}},{\"id\":\"multi-scale-and-multi-level-attention-based-on-external-knowledge-in-ehrs\",\"rank\":\"B\",\"title\":\"Multi-scale and Multi-level Attention based on External knowledge in EHRs\",\"authors\":[\"Duc Le\",\"Bac Le\"],\"type\":\"Conference\",\"isFirstAuthor\":true,\"highlight\":\"Proposes a multi-level, multi-scale attention model incorporating external knowledge to improve risk prediction tasks on EHRs.\",\"venue\":\"Asian Conference on Intelligent Information and Database Systems\",\"abbreviation\":\"ACIIDS\",\"year\":\"2024-04-01\",\"abstract\":\"This paper proposes a multi-level, multi-scale attention model utilizing external knowledge to enhance diagnostic code prediction from longitudinal Electronic Health Records (EHRs). The model exploits hierarchical relationships among codes and mimics doctors' reasoning from general to detailed levels, achieving more than 2% F1-score improvement over baselines on the MIMIC-IV dataset.\",\"problem\":[{\"type\":\"text\",\"text\":\"Deep learning models often fail to exploit hierarchical relationships between diagnostic codes effectively, leading to dispersed attention across a large code space and bias towards frequently occurring codes.\"},{\"type\":\"text\",\"text\":\"Current EHR prediction models mostly focus on temporal modeling, ignoring rich external knowledge that could enhance interpretability and accuracy.\"}],\"gap\":[{\"type\":\"text\",\"text\":\"Most existing methods do not sufficiently integrate external medical knowledge like hierarchical code relations (e.g., CCSR structure) into the model's attention mechanism.\"},{\"type\":\"text\",\"text\":\"Interpretability is often compromised when trying to optimize model accuracy, creating a gap between model predictions and clinical trust.\"}],\"solution\":[{\"type\":\"text\",\"text\":\"We propose a Multi-scale and Multi-level Attention model that incorporates external hierarchical knowledge (CCSR), organizes diagnostic codes into meaningful levels, and leverages a single feature encoder to model general to detailed correlations.\"},{\"type\":\"image\",\"url\":\"https://upload.wikimedia.org/wikipedia/commons/8/88/Transformer.jpg\",\"caption\":\"Overview of multi-level and multi-scale attention model architecture (illustrative)\"},{\"type\":\"text\",\"text\":\"The model uses Multi-level Attention across CCSR body systems, CCSR categories, and ICD-10 codes, combined with a Multi-scale Feature Synthesizer and Time-aware Dynamic Attention Fusion for enhanced risk prediction.\"}],\"results\":[{\"type\":\"text\",\"text\":\"The model achieves over 2% F1-score improvement compared to strong baselines like HiTANet on the MIMIC-IV dataset, confirming the effectiveness of multi-level hierarchical integration.\"},{\"type\":\"text\",\"text\":\"Multi-scale attention fusion improves prediction consistency and interpretability by tracing attention allocation across patient visits and code groups.\"}],\"insights\":[{\"type\":\"text\",\"text\":\"Global attention mechanisms capture long-term health trends, while local attention mechanisms focus on visit-specific details, allowing the model to explain predictions at different abstraction levels.\"},{\"type\":\"text\",\"text\":\"Incorporating external hierarchy (CCSR) effectively narrows the model's focus range, making training more stable and predictions more interpretable.\"}],\"contributions\":[{\"type\":\"text\",\"text\":\"1. Introduced a multi-scale, multi-level attention mechanism that improves EHR-based risk prediction accuracy while enhancing interpretability.\"},{\"type\":\"text\",\"text\":\"2. Demonstrated the utility of CCSR-based external knowledge in organizing diagnostic codes into hierarchies for better model training.\"},{\"type\":\"text\",\"text\":\"3. Provided a comprehensive experimental evaluation showing consistent improvements across multiple metrics compared to existing baselines.\"}],\"topics\":[\"Healthcare AI\",\"Predictive Modeling\",\"Deep Learning\",\"Attention Mechanisms\",\"Electronic Health Records\"],\"doi\":null,\"links\":{\"website\":null,\"youtube_demo\":null,\"github_repository\":\"https://github.com/Haru-Lab-Space/MsTA\",\"view_publication\":\"https://ceur-ws.org/Vol-3658/paper5.pdf\"},\"citationCount\":0,\"citationFormat\":\"Le, D., \u0026 Le, B. (2024). Multi-scale and Multi-level Attention based on External knowledge in EHRs. International Conference on Multimedia Information Processing and Retrieval (MIPR) 2024.\",\"images\":[],\"technologies\":[\"PyTorch\",\"Transformer\",\"Attention Mechanism\",\"Healthcare Data Analysis\"],\"references\":{\"1\":\"Ye, M., et al. MedPath: Augmenting Health Risk Prediction via Medical Knowledge Paths. WWW 2021.\",\"2\":\"Choi, E., et al. GRAM: Graph-Based Attention Model for Healthcare Representation Learning. KDD 2017.\",\"3\":\"Luo, J., et al. HiTANet: Hierarchical Time-Aware Attention Networks for Risk Prediction. KDD 2020.\"}}],\"yourName\":\"Duc Le\",\"sectionTitle\":\"Conference Publications\",\"rankOrder\":{\"CORE A*\":1,\"A*\":1,\"CORE A\":2,\"A\":2,\"CORE B\":3,\"B\":3,\"CORE C\":4,\"C\":4,\"Q1\":1,\"Q2\":2,\"Q3\":3,\"Q4\":4}}],[\"$\",\"$L18\",null,{\"publications\":[],\"yourName\":\"Duc Le\",\"sectionTitle\":\"Journal Publications\",\"rankOrder\":\"$a:props:children:1:props:children:props:children:0:props:rankOrder\"}],[\"$\",\"$L18\",null,{\"publications\":[{\"id\":\"handle-the-problem-of-ample-label-space-by-using-the-image-guided-feature-extractor-on-the-musti-dataset\",\"rank\":\"B\",\"title\":\"Handle the problem of ample label space by using the Image-guided Feature Extractor on the MUSTI dataset\",\"authors\":[\"Le Ngoc-Duc\",\"Le Minh-Hung\",\"Dinh Quang-Vinh\"],\"type\":\"Workshop\",\"isFirstAuthor\":true,\"highlight\":\"Proposed an Image-guided Feature Extractor model that improves text-image retrieval performance in the field of olfactory perception.\",\"venue\":\"MediaEval 2023 Workshop\",\"abbreviation\":\"MMM\",\"year\":\"2023-02-01\",\"abstract\":\"This paper introduces the Image-guided Feature Extractor model to address the challenges of large label space and data imbalance in predicting alignment between images and olfactory-related text descriptions on the MUSTI dataset.\",\"problem\":[{\"type\":\"text\",\"text\":\"In multimedia tasks, olfactory perception is a rarely explored area. Major challenges include a large label space, small dataset size, and class imbalance.\"},{\"type\":\"text\",\"text\":\"Extracting features from image and text independently results in rich representations but makes it hard for the model to focus on relevant content.\"}],\"gap\":[{\"type\":\"text\",\"text\":\"Previous approaches extract multimodal features in a fragmented manner, often causing confusion for models due to a lack of guided attention.\"}],\"solution\":[{\"type\":\"text\",\"text\":\"We propose the Image-guided Feature Extractor, which uses image information to guide textual feature extraction, helping the model focus on the most relevant parts of the input and improve performance.\"}],\"results\":[{\"type\":\"text\",\"text\":\"The proposed model outperforms baseline methods in predicting alignment between images and olfactory perception-related text on the MUSTI dataset.\"}],\"insights\":[{\"type\":\"text\",\"text\":\"Using visual information to guide textual feature extraction enables the model to focus better and generalize well in low-resource multimodal settings.\"}],\"contributions\":[{\"type\":\"text\",\"text\":\"1. Proposed an Image-guided Feature Extractor model that enhances performance in text-image retrieval tasks.\"},{\"type\":\"text\",\"text\":\"2. Addressed the issues of large label space and class imbalance in the olfactory perception domain.\"},{\"type\":\"text\",\"text\":\"3. Introduced a new research direction in cross-modal learning between vision and olfactory-related textual data.\"}],\"topics\":[\"Multimodal AI\",\"Computer Vision\",\"Feature Engineering\",\"Natural Language Processing\"],\"doi\":null,\"links\":{\"website\":null,\"youtube_demo\":null,\"github_repository\":\"https://github.com/Haru-Lab-Space/MMM2024.git\",\"view_publication\":\"https://ceur-ws.org/Vol-3658/paper5.pdf\"},\"citationCount\":0,\"citationFormat\":\"Le, N.-D., Le, M.-H., \u0026 Dinh, Q.-V. (2023). Handle the problem of ample label space by using the Image-guided Feature Extractor on the MUSTI dataset. In MediaEval 2023 Workshop.\",\"images\":[],\"technologies\":[\"PyTorch\",\"Hugging Face Transformers\",\"CLIP\",\"Cross-Attention\",\"Vision-Language Models\",\"Image-to-Text\"],\"references\":{}}],\"yourName\":\"Duc Le\",\"sectionTitle\":\"Workshop Publications\",\"rankOrder\":\"$a:props:children:1:props:children:props:children:0:props:rankOrder\"}],false]}]}]]}]\n"])</script></body></html>