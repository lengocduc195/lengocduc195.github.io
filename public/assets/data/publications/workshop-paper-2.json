{
  "id": 11,
  "rank": "B",
  "title": "Adversarial Robustness in Vision-Language Models",
  "authors": [
    "James Wilson",
    "Duc Le",
    "Mei Zhang"
  ],
  "type": "Workshop",
  "isFirstAuthor": false,
  "venue": "Workshop on Adversarial Machine Learning at ICLR",
  "year": 2022,
  "abstract": "This paper investigates the adversarial robustness of vision-language models, identifying vulnerabilities and proposing defense mechanisms.",
  "content": "This paper investigates the adversarial robustness of vision-language models, identifying vulnerabilities and proposing defense mechanisms. We conduct a systematic evaluation of popular vision-language models against various adversarial attacks targeting either the visual or textual components. Our findings reveal that these models are particularly vulnerable to cross-modal attacks that exploit inconsistencies between visual and textual representations. We propose a novel adversarial training approach that specifically addresses these cross-modal vulnerabilities, significantly improving robustness while maintaining performance on clean data.",
  "topics": [
    "Adversarial Robustness",
    "Vision-Language Models",
    "Multimodal Learning",
    "Security",
    "Deep Learning"
  ],
  "doi": "10.48550/arXiv.2203.12345",
  "link": "https://openreview.net/forum?id=abcdefg12345",
  "citationCount": 15,
  "citationFormat": "Wilson, J., Le, D., & Zhang, M. (2022). Adversarial Robustness in Vision-Language Models. In Workshop on Adversarial Machine Learning at ICLR 2022.",
  "videoUrl": "https://www.youtube.com/watch?v=wxyz98765",
  "github": "https://github.com/james-wilson/robust-vlm",
  "images": [
    {
      "url": "/images/robust-vlm.jpg",
      "caption": "Adversarial Examples for Vision-Language Models"
    }
  ],
  "technologies": ["Python", "PyTorch", "CLIP", "Hugging Face", "Foolbox"]
}
